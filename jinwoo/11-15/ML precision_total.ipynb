{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899e7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733846a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('C:/Users/loste/data_1112/ir1.csv',  encoding= 'CP949')\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for data in a :\n",
    "    x.append(a[data].tolist())\n",
    "    y.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa82d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('C:/Users/loste/data_1112/ir2.csv', encoding= 'CP949')\n",
    "#print(a['one'].tolist())\n",
    "for data in b :\n",
    "    x.append(b[data].tolist())\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bbb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.read_csv('C:/Users/loste/data_1112/red1.csv',  encoding= 'CP949')\n",
    "\n",
    "z=[]\n",
    "w=[]\n",
    "for data in c :\n",
    "    z.append(c[data].tolist())\n",
    "    w.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a96492",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('C:/Users/loste/data_1112/red2.csv', encoding= 'CP949')\n",
    "#print(a['one'].tolist())\n",
    "for data in d :\n",
    "    z.append(d[data].tolist())\n",
    "    w.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42296f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "z = np.array(z)\n",
    "w = np.array(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca95c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, \n",
    "\n",
    "                                                    y, \n",
    "\n",
    "                                                    test_size=0.3, \n",
    "\n",
    "                                                    shuffle=True, \n",
    "\n",
    "                                                    random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e88df46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "z_train, z_test, w_train, w_test = train_test_split(z, \n",
    "\n",
    "                                                    w, \n",
    "\n",
    "                                                    test_size=0.3, \n",
    "\n",
    "                                                    shuffle=True, \n",
    "\n",
    "                                                    random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d41c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.16666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.50      0.29         2\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.17         6\n",
      "   macro avg       0.10      0.25      0.14         6\n",
      "weighted avg       0.07      0.17      0.10         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x_train,y_train)\n",
    "print(\"train score:\", log_reg.score(x_train, y_train))\n",
    "x_score = log_reg.score(x_test, y_test)\n",
    "print(\"test score:\", x_score)\n",
    "y_pred = log_reg.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dbb4c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.16666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.50      0.29         2\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.17         6\n",
      "   macro avg       0.10      0.25      0.14         6\n",
      "weighted avg       0.07      0.17      0.10         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(z_train,w_train)\n",
    "print(\"train score:\", log_reg.score(z_train, w_train))\n",
    "z_score = log_reg.score(z_test, w_test)\n",
    "print(\"test score:\", z_score)\n",
    "w_pred = log_reg.predict(z_test)\n",
    "print(classification_report(w_test, w_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb8dc5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression average test score:  0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression average test score: \", (x_score + z_score) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64afa085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         2\n",
      "           1       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.50      0.50      0.49         6\n",
      "weighted avg       0.56      0.50      0.51         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=32) #decision_tree변수에 모델 저장\n",
    "decision_tree.fit(x_train, y_train) #fit메서드로 모델 학습\n",
    "\n",
    "print(\"train score:\", decision_tree.score(x_train, y_train))\n",
    "x_score = decision_tree.score(x_test, y_test)\n",
    "print(\"test score:\", x_score)\n",
    "y_pred = decision_tree.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19186da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.16666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.50      0.29         2\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.17         6\n",
      "   macro avg       0.10      0.25      0.14         6\n",
      "weighted avg       0.07      0.17      0.10         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=32) #decision_tree변수에 모델 저장\n",
    "decision_tree.fit(z_train, w_train) #fit메서드로 모델 학습\n",
    "\n",
    "print(\"train score:\", decision_tree.score(z_train, w_train))\n",
    "z_score = decision_tree.score(z_test, w_test)\n",
    "print(\"test score:\", z_score)\n",
    "w_pred = decision_tree.predict(z_test)\n",
    "print(classification_report(w_test, w_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5c3ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree average test score:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision tree average test score: \", (x_score + z_score) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4587f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00         6\n",
      "   macro avg       1.00      1.00      1.00         6\n",
      "weighted avg       1.00      1.00      1.00         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(x_train, y_train)\n",
    "print(\"train score:\", classifier.score(x_train, y_train))\n",
    "x_score = classifier.score(x_test, y_test)\n",
    "print(\"test score:\", x_score)\n",
    "y_pred = classifier.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e87f4414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9285714285714286\n",
      "test score: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         2\n",
      "           1       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.75      0.75      0.67         6\n",
      "weighted avg       0.83      0.67      0.67         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(z_train, w_train)\n",
    "print(\"train score:\", classifier.score(z_train, w_train))\n",
    "z_score = classifier.score(z_test, w_test)\n",
    "print(\"test score:\", z_score)\n",
    "w_pred = classifier.predict(z_test)\n",
    "print(classification_report(w_test, w_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f02944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian average test score:  0.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Gaussian average test score: \", (x_score + z_score) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29da658b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7857142857142857\n",
      "test score: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         2\n",
      "           1       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.75      0.75      0.67         6\n",
      "weighted avg       0.83      0.67      0.67         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 2)\n",
    "classifier.fit(x_train, y_train)\n",
    "print(\"train score:\", classifier.score(x_train, y_train))\n",
    "x_score = classifier.score(x_test, y_test)\n",
    "print(\"test score:\", x_score)\n",
    "y_pred = classifier.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "626ff5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8571428571428571\n",
      "test score: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         2\n",
      "           1       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.75      0.75      0.67         6\n",
      "weighted avg       0.83      0.67      0.67         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 2)\n",
    "classifier.fit(z_train, w_train)\n",
    "print(\"train score:\", classifier.score(z_train, w_train))\n",
    "z_score = classifier.score(z_test, w_test)\n",
    "print(\"test score:\", z_score)\n",
    "w_pred = classifier.predict(z_test)\n",
    "print(classification_report(w_test, w_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7adf231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Neigbors average test score:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"K Neigbors average test score: \", (x_score + z_score) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8598e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      1.00      0.50         2\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.33         6\n",
      "   macro avg       0.17      0.50      0.25         6\n",
      "weighted avg       0.11      0.33      0.17         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loste\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\loste\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\loste\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='poly')\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"train score:\", clf.score(x_train, y_train))\n",
    "x_score = clf.score(x_test, y_test)\n",
    "print(\"test score:\", x_score)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74ef798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      1.00      0.50         2\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.33         6\n",
      "   macro avg       0.17      0.50      0.25         6\n",
      "weighted avg       0.11      0.33      0.17         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loste\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\loste\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\loste\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='poly')\n",
    "clf.fit(z_train, w_train)\n",
    "print(\"train score:\", clf.score(z_train, w_train))\n",
    "z_score = clf.score(z_test, w_test)\n",
    "print(\"test score:\", z_score)\n",
    "w_pred = clf.predict(z_test)\n",
    "print(classification_report(w_test, w_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73b29d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM average test score:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM average test score: \", (x_score + z_score) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae5c738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.8333333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.83      0.88      0.83         6\n",
      "weighted avg       0.89      0.83      0.84         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = 50 ,n_estimators=100, oob_score=True, random_state=123456)\n",
    "rf.fit(x_train, y_train)\n",
    "print(\"train score:\", rf.score(x_train, y_train))\n",
    "x_score = rf.score(x_test, y_test)\n",
    "print(\"test score:\", x_score)\n",
    "y_pred = rf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a891f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         2\n",
      "           1       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.50      0.50      0.49         6\n",
      "weighted avg       0.56      0.50      0.51         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = 50 ,n_estimators=100, oob_score=True, random_state=123456)\n",
    "rf.fit(z_train, w_train)\n",
    "print(\"train score:\", rf.score(z_train, w_train))\n",
    "z_score = rf.score(z_test, w_test)\n",
    "print(\"test score:\", z_score)\n",
    "w_pred = rf.predict(z_test)\n",
    "print(classification_report(w_test, w_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "840344a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest average test score:  0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"RandomForest average test score: \", (x_score + z_score) / 2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
