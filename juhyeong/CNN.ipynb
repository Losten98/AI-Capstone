{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed274574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37e793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_data(Dataset):\n",
    "    def __init__(self, path, option):\n",
    "            data = pd.read_csv(path,header=None)\n",
    "            \n",
    "            self.items = [(data[i] ,int(path.split(option)[-1].replace(\".csv\",\"\"))) for i in data]\n",
    "            self.length = len(data.columns) # item의 개수\n",
    "            self.to_tensor = transforms.ToTensor()\n",
    "            \n",
    "    # ----------------------------\n",
    "    # Number of items in dataset\n",
    "    # ----------------------------\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Get i'th item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        series_data, class_id = self.items[idx]\n",
    "        \n",
    "        eps=0.1\n",
    "        steps=5\n",
    "        N = series_data.size\n",
    "        S = np.repeat(series_data[None,:],N, axis=0)\n",
    "        image = np.floor(np.abs(S-S.T)/eps)\n",
    "        image[image>steps] = steps\n",
    "        image = self.to_tensor(image)\n",
    "        \n",
    "        \n",
    "        return image, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6aa1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/wonHR1.csv'\n",
    "train = img_data(PATH,\"HR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c59a9444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4155440/1448392316.py:25: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  S = np.repeat(series_data[None,:],N, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.__getitem__(1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2570d488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4155440/1448392316.py:25: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  S = np.repeat(series_data[None,:],N, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff8ebb8f280>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train.__getitem__(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a966be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "batch_size = 1\n",
    "num_epochs=1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631e275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_esc50, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_esc50, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e8aa245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class Classifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=2)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "        # Dropout\n",
    "        # self.dropout = nn.Dropout(0.25)\n",
    "\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Dropout\n",
    "        # x = self.dropout(x) \n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67cf6755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model and put it on the GPU if available\n",
    "myModel = Classifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fddc53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "    print('Starting Training')\n",
    "    # Repeat for each epoch\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        model.train()\n",
    "\n",
    "        # Repeat for each batch in the training set\n",
    "        for inputs, labels in train_dl:\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             # Normalize the inputs\n",
    "#             inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "#             inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss.item())\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "            # if i % 10 == 0:    # print every 10 mini-batches\n",
    "            #     print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "\n",
    "\n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dl.dataset)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction/total_prediction\n",
    "        pbar.set_postfix({'Epoch':epoch, 'Train_loss':avg_loss})\n",
    "        # print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dadb45b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, test_dl):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_dl):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e34ae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb931832e9384ceb8c9735825c138723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3757453/1448392316.py:25: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  S = np.repeat(series_data[None,:],N, axis=0)\n",
      "/tmp/ipykernel_3757453/1448392316.py:25: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  S = np.repeat(series_data[None,:],N, axis=0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (double) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Train 시작 시간 정보 저장\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# Train 종료 시간 정보 저장\u001b[39;00m\n\u001b[1;32m      6\u001b[0m duration \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start \u001b[38;5;66;03m# 종료 시간 - 시작 시간\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [50], line 36\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, train_dl, num_epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/ai_capstone/.ai_capstone/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [48], line 60\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Run the convolutional blocks\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Adaptive pool and flatten for input to linear layer\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39map(x)\n",
      "File \u001b[0;32m~/ai_capstone/.ai_capstone/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ai_capstone/.ai_capstone/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/ai_capstone/.ai_capstone/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ai_capstone/.ai_capstone/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai_capstone/.ai_capstone/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (double) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "start = time.time() # Train 시작 시간 정보 저장\n",
    "training(myModel, train_loader, num_epochs)\n",
    "end = time.time() # Train 종료 시간 정보 저장\n",
    "\n",
    "duration = end - start # 종료 시간 - 시작 시간\n",
    "print(\"Training takes {:.2f} minutes\".format(duration/60)) #초 단위로 저장되므로, 60으로 나누어 분으로 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb9d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_capstone",
   "language": "python",
   "name": ".ai_capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
